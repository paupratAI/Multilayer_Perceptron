{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 1 - Xarxes Neuronals i Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document realitzat pels alumnes Pau Prat i Violeta Bonet, del grau en Inteligència Artificial (UPC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lloc, cal destacar que aquesta primera cel·la conté tots els mòduls i llibreries necessàries per a l'execució del codi. \n",
    "\n",
    "Per tant, en cas de no tenir-les, caldrà executar aquesta cel·la prèviament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%pip install --upgrade pip  --quiet\n",
    "%pip install pandas  --upgrade --quiet\n",
    "%pip install numpy  --upgrade --quiet\n",
    "%pip install scipy  --upgrade --quiet\n",
    "%pip install statsmodels  --upgrade --quiet\n",
    "%pip install seaborn  --upgrade --quiet\n",
    "%pip install scikit-learn==1.3.0\n",
    "%pip install tqdm ipykernel matplotlib ipywidgets --upgrade --quiet   \n",
    "%pip install plotly numpy==1.25 nbformat umap-learn\n",
    "%pip install ucimlrepo\n",
    "%pip install mlxtend\n",
    "%pip install pydotplus\n",
    "%pip install imbalanced-learn\n",
    "%pip install yellowbrick\n",
    "%pip install missingno\n",
    "%pip install tensorflow\n",
    "%load_ext autoreload\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports bàsics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definim la mida predeterminada dels gràfics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [9, 6]  \n",
    "\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "sns.set(font_scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Anàlisi Exploratòria de Dades (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:lightgreen\"> Carreguem la base de dades</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"smartphone_data.csv\")\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_features(df, target): \n",
    "    initial_numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    initial_categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    numerical_features = []\n",
    "    categorical_features = initial_categorical_features  \n",
    "\n",
    "    for column in initial_numerical_features:\n",
    "        if column != target:\n",
    "            if df[column].nunique() < 10:\n",
    "                categorical_features.append(column) \n",
    "            else:\n",
    "                numerical_features.append(column)  \n",
    "\n",
    "    def feature_type(column):\n",
    "        if column in numerical_features:\n",
    "            return 'Numerical'\n",
    "        elif column in categorical_features:\n",
    "            return 'Categorical'\n",
    "        else:\n",
    "            return 'Boolean'\n",
    "\n",
    "    features = pd.DataFrame({\n",
    "        'Feature': [column for column in df.columns if column != 'price'],\n",
    "        'Type': [df[column].dtype for column in df.columns if column != 'price'],\n",
    "        'Unique values': [df[column].nunique() for column in df.columns if column != 'price'],\n",
    "        'Category': [feature_type(column) for column in df.columns if column != 'price']\n",
    "    })\n",
    "\n",
    "    features.sort_values(by='Unique values', ascending=True, inplace=True)\n",
    "    return numerical_features, categorical_features, features\n",
    "\n",
    "numerical_features, categorical_features, features = classify_features(df, 'price')\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in ['has_5g', 'has_nfc', 'has_ir_blaster']:\n",
    "    df[variable] = df[variable].map({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features, categorical_features, features = classify_features(df, 'price')\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar 'model' de la llista de variables categòriques i de 'df'\n",
    "categorical_features.remove('model')\n",
    "df.drop('model', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:lightgreen\"> Visualitzem la distribució de cada variable numèrica</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 2\n",
    "num_cols = 4\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize = (12, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes[:len(numerical_features)]):\n",
    "    sns.histplot(df[numerical_features[i]], bins = 30, color = 'blue', edgecolor = 'white', kde = True, ax = ax)\n",
    "\n",
    "# Eliminar els subplots sobrants\n",
    "for ax in axes[len(numerical_features):]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('./plots/num_dist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cas que es vulgui obtenir el gràfic de la distribució de cada variable numèrica per separat, simplement cal descomentar i executar la següent cel·la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for feature in numerical_features:\n",
    "    mean = df[feature].mean()\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.histplot(df[feature], kde=False, ax=ax, edgecolor=\"black\")\n",
    "    ax.plot([mean], [-0.6], marker='^', markersize=9, color=\"red\")\n",
    "    ax.set_title(f'Distribució de {feature}')\n",
    "    ax.set_xlabel(feature, size=10)\n",
    "    ax.set_ylabel(\"Freqüència\", size=10)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f'./plots/{feature}_distribution.png')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:lightgreen\"> Histograma de la freqüència per classe de cada variable categòrica </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 4, figsize = (16, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.countplot(data = df, x = categorical_features[i], color = 'green', ax = ax)\n",
    "    if categorical_features[i] in ['resolution', 'processor_brand', 'brand_name']:\n",
    "        ax.set_xticklabels([])  \n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('./plots/hist_freq.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cas que es vulgui obtenir el gràfic de la freqüència per classe de cada variable categòrica per separat, simplement cal descomentar i executar la següent cel·la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for feature in categorical_features:\n",
    "    plt.figure()\n",
    "    df[feature].value_counts().plot(kind='bar', color='green')\n",
    "    plt.title(f'Freqüència per classe - {feature}')\n",
    "    if feature == 'resolution':\n",
    "        plt.xticks(rotation=90, fontsize=8)  \n",
    "    else:\n",
    "        plt.xticks(rotation=70, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f'./plots/{feature}_frequency.png')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:lightgreen\"> Correlacions entre variables numèriques </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df[numerical_features].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
    "plt.tight_layout()  \n",
    "#plt.savefig('./plots/correlations_heatmap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:lightgreen\"> Correlació entre variables categòriques i variable objectiu </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertim les unitats de 'price' de rúpies a euros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicar els valors de price per 0.011\n",
    "df['price'] = df['price'] * 0.011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualitzar la distribució de 'price'\n",
    "plt.figure()\n",
    "sns.histplot(df['price'], bins = 30, color = 'blue', edgecolor = 'white', kde = True)\n",
    "plt.title('Distribució de price')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Freqüència')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('./plots/price_distribution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, feature in enumerate(categorical_features):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.boxplot(x=feature, y='price', data=df)\n",
    "    plt.title(f'Distribució de price segons {feature}')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()  \n",
    "    #plt.savefig(f'./plots/boxplots/{feature}.png')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:lightgreen\"> Correlació entre variables numèriques i variable objectiu </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "num_rows = 2\n",
    "num_cols =  4\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize = (14, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    sns.regplot(x=feature, y='price', data=df, scatter_kws={'alpha':0.5}, line_kws={\"color\": \"red\"}, ax=axes[i])  # alpha para transparencia de puntos\n",
    "    axes[i].set_title(f'Relació entre Price i {feature}')  \n",
    "    axes[i].set_xlabel(feature) \n",
    "    axes[i].set_ylabel('Price')  \n",
    "\n",
    "# Eliminar els subplots sobrants\n",
    "for ax in axes[len(numerical_features):]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('./plots/numerical_correlations.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessament"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:lightgreen\"> Missings </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "plt.figure()\n",
    "msno.matrix(df)\n",
    "plt.tight_layout()  \n",
    "#plt.savefig('./plots/missingno_matrix.png')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data(data):\n",
    "    total_missing = data.isna().sum().sort_values(ascending=False)\n",
    "    percent_missing = round(100 * (data.isnull().sum() / len(data)), 2).sort_values(ascending=False)\n",
    "    missing_data = pd.DataFrame({'Total Missing': total_missing, 'Percent Missing (%)': percent_missing})\n",
    "    return missing_data\n",
    "missing_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminem 'extended_upto' ja que té gairebé un 50% de missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features.remove('extended_upto')\n",
    "df = df.drop('extended_upto', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Els altres missings els imputarem un cop particionem el dataset en train i test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:lightgreen\"> Outliers </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in numerical_features:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Plot boxplot\n",
    "    sns.boxplot(x=df[feature], ax=axes[0])\n",
    "    axes[0].set_title(f'{feature} with outliers')\n",
    "\n",
    "    # Plot distribution\n",
    "    sns.histplot(data=df, x=feature, kde=True, ax=axes[1])\n",
    "    axes[1].set_title(f'{feature} Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #fig.savefig(f'./plots/dist_with_outliers/{feature}_with_outliers.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminarem els outliers seguint el criteri del Rang Interquartil, excepte la variable objectiu 'price'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per tant, considerarem outliers:\n",
    "* Els valors més grans que Q1 - 1.5*IQR\n",
    "* Els valors més petits que Q3 + 1.5*IQR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in numerical_features:\n",
    "    # Calcular IQR\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Definir el límit inferior i superior\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Replace outliers with NaN\n",
    "    df.loc[(df[feature] < lower_bound) | (df[feature] > upper_bound), feature] = np.nan\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Plot boxplot\n",
    "    sns.boxplot(x=df[feature], ax=axes[0])\n",
    "    axes[0].set_title(f'{feature} Boxplot')\n",
    "\n",
    "    # Plot distribution\n",
    "    sns.histplot(data=df, x=feature, kde=True, ax=axes[1])\n",
    "    axes[1].set_title(f'{feature} Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #fig.savefig(f'./plots/dist_without_outliers/{feature}_without_outliers.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar files tal que 'price' es un outlier\n",
    "Q1 = df['price'].quantile(0.10)\n",
    "Q3 = df['price'].quantile(0.90)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df = df[(df['price'] >= lower_bound) & (df['price'] <= upper_bound)]\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara, tornem a observar la distribució de 'price' i podem comprovar que els valors ja són més típics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualitzar la distribució de 'price'\n",
    "plt.figure()\n",
    "sns.histplot(df['price'], bins = 30, color = 'blue', edgecolor = 'white', kde = True)\n",
    "plt.title('Distribució de price')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Freqüència')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('./plots/price_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Remostreig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:lightgreen\"> Partició del dataset en Train i Test </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('price', axis=1) \n",
    "y = df['price']\n",
    "\n",
    "# Dividir el dataset en train y test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el número d'observacions y features de cada set\n",
    "sets_df = pd.DataFrame(columns=['Set', 'Number of Observations', 'Number of Features'])\n",
    "sets_df.loc[len(sets_df)] = ['Train', X_train.shape[0], X_train.shape[1]]\n",
    "sets_df.loc[len(sets_df)] = ['Test', X_test.shape[0], X_test.shape[1]]\n",
    "\n",
    "sets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:lightgreen\"> Imputació de Missings</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variables numèriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_before_num = X_train[numerical_features].isnull().sum()\n",
    "mean_before = X_train[numerical_features].mean()\n",
    "stderr_before = X_train[numerical_features].sem()\n",
    "median_before = X_train[numerical_features].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Crear l'imputador KNN\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Ajustar l'imputador a les característiques numèriques de les dades d'entrenament i transformar-les\n",
    "X_train[numerical_features] = imputer.fit_transform(X_train[numerical_features])\n",
    "\n",
    "# Transformar les característiques numèriques de les dades de prova utilitzant l'imputador ajustat\n",
    "X_test[numerical_features] = imputer.transform(X_test[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_after_num = X_train[numerical_features].isnull().sum()\n",
    "mean_after = X_train[numerical_features].mean()\n",
    "stderr_after = X_train[numerical_features].sem()\n",
    "median_after = X_train[numerical_features].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_comparison_num = pd.DataFrame({\n",
    "    'Feature': missing_before_num.index, \n",
    "    'Mean (old)': mean_before.values,\n",
    "    'Mean': mean_after.values,\n",
    "    'Std_Error (old)': stderr_before.values,\n",
    "    'Std_Error': stderr_after.values,\n",
    "    'Median (old)': median_before.values,\n",
    "    'Median': median_after.values\n",
    "})\n",
    "stats_comparison_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variables categòriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar les dades originals\n",
    "original_data = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llista amb els noms de les variables categòriques que tenen almenys un missing\n",
    "cat_features_missings = [feature for feature in categorical_features if X_train[feature].isnull().any()]\n",
    "print(cat_features_missings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "X_train[categorical_features] = imputer.fit_transform(X_train[categorical_features])\n",
    "X_test[categorical_features] = imputer.transform(X_test[categorical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in cat_features_missings:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # Abans de la imputació (excloïm missings)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    filtered_data = original_data[original_data[feature].notna()]\n",
    "    sns.countplot(data=filtered_data, x=feature, color='green')\n",
    "    plt.title(f'{feature} - Original')\n",
    "    plt.xticks(rotation=60)  \n",
    "\n",
    "    # Després de la imputació\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.countplot(data=X_train, x=feature, color='green')\n",
    "    plt.title(f'{feature} - Després de la Imputació')\n",
    "    plt.xticks(rotation=60)  \n",
    "    plt.tight_layout()  \n",
    "    #plt.savefig(f'./plots/dist_moda/{feature}_moda.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:lightgreen\"> Recodificació de variables categòriques </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Crear una còpia dels conjunts de dades per no modificar els originals\n",
    "X_train_encoded = X_train.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "# Crear el codificador\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Obtindre totes les categories úniques per a 'brand_name' i 'processor_brand'\n",
    "all_brands = pd.concat([X_train['brand_name'], X_test['brand_name']]).dropna().unique()\n",
    "all_processors = pd.concat([X_train['processor_brand'], X_test['processor_brand']]).dropna().unique()\n",
    "\n",
    "# One-Hot Encoding per 'brand_name' i 'processor_brand'\n",
    "for col, all_categories in zip(['brand_name', 'processor_brand'], [all_brands, all_processors]):\n",
    "    dummies = pd.get_dummies(pd.concat([X_train[col], X_test[col]], axis=0), prefix=col, drop_first=True)\n",
    "    X_train_encoded = pd.concat([X_train_encoded, dummies.loc[X_train.index]], axis=1)\n",
    "    X_test_encoded = pd.concat([X_test_encoded, dummies.loc[X_test.index]], axis=1)\n",
    "    X_train_encoded.drop([col], axis=1, inplace=True)\n",
    "    X_test_encoded.drop([col], axis=1, inplace=True)\n",
    "\n",
    "# Codificar les altres columnes categòriques de tipus object\n",
    "for col in categorical_features:\n",
    "    if col not in ['brand_name', 'processor_brand'] and X_train_encoded[col].dtype == 'object':\n",
    "        le.fit(pd.concat([X_train_encoded[col], X_test_encoded[col]])) \n",
    "        X_train_encoded[col] = le.transform(X_train_encoded[col])\n",
    "        X_test_encoded[col] = le.transform(X_test_encoded[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:lightgreen\"> Normalització de variables numèriques </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Crear una còpia dels conjunts de dades per no modificar els originals\n",
    "X_train_normalized = X_train_encoded.copy()\n",
    "X_test_normalized = X_test_encoded.copy()\n",
    "\n",
    "# Crear el normalitzador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalitzar les columnes numèriques\n",
    "X_train_normalized[numerical_features] = scaler.fit_transform(X_train_normalized[numerical_features])\n",
    "X_test_normalized[numerical_features] = scaler.transform(X_test_normalized[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    X_train_normalized[feature].plot.hist()\n",
    "    plt.title(f'Distribució Normalitzada - {feature}')\n",
    "    #plt.savefig(f'./plots/normalized/{feature}_normalized.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:lightgreen\"> CV per avaluar el model </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Model Lineal Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:lightgreen\"> Entrenament i avaluació d'un model de regressió lineal </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Crea el model de regressió lineal\n",
    "model = LinearRegression()\n",
    "\n",
    "# Entrena el model\n",
    "model.fit(X_train_normalized, y_train)\n",
    "\n",
    "# Fes prediccions amb les dades de prova\n",
    "y_pred = model.predict(X_test_normalized)\n",
    "\n",
    "# Avaluar el model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R²: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:lightgreen\"> Interpretació dels resultats obtinguts (mètriques de classificació, coeficients, etc.) </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearity(model, X, y):\n",
    "    \"\"\"\n",
    "    Funció per visualitzar la linealitat entre els valors predits i els reals.\n",
    "    \"\"\"\n",
    "    # Generar prediccions\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Crear un gràfic de dispersió dels valors reals vs. predits\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(y, y_pred, alpha=0.5)\n",
    "    plt.title('Comparació entre els Valors Reals i els Predits')\n",
    "    plt.xlabel('Valors Reals')\n",
    "    plt.ylabel('Valors Predits')\n",
    "\n",
    "    # Dibuixar la línia diagonal que representa la perfecta predicció\n",
    "    max_val = max(np.max(y), np.max(y_pred))\n",
    "    min_val = min(np.min(y), np.min(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--')\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "linearity(model, X_test_normalized, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 EXTRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trobar el millor model per aquest problema de regressió mitjançant validació creuada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Crear un diccionari de models a avaluar\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(max_iter=10000),\n",
    "    'Lasso Regression': Lasso(max_iter=10000),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'K-Nearest Neighbors': make_pipeline(StandardScaler(), KNeighborsRegressor()) \n",
    "}\n",
    "\n",
    "# Configuració de la validació creuada\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Avaluar cada model fent servir el R2 Score\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    cv_results = cross_val_score(model, X_train_normalized, y_train, cv=kf, scoring='r2')\n",
    "    results[name] = cv_results.mean()  \n",
    "\n",
    "# Trobar el model que dona un millor resultat de R2\n",
    "best_model = max(results, key=results.get)\n",
    "print(\"Millor model:\", best_model)\n",
    "print(\"R2-score de cada model:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Definir el model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Definir els hiperparàmetres a provar\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Crear el Grid Search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=10, scoring='r2', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Ajustar el Grid Search als dades\n",
    "grid_search.fit(X_train_normalized, y_train)\n",
    "\n",
    "# Imprimir els millors hiperparàmetres trobats\n",
    "print(f'Millors hiperparàmetres: {grid_search.best_params_}')\n",
    "print(f'Millor score: {grid_search.best_score_}')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El codi anterior té un cost computacional molt elevat, per tant, com que els resultats són reproductibles (random_state), els hiperparàmetres òptims obtinguts són els següents:\n",
    "-  {'bootstrap': True, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
    "\n",
    "En cas que es vulgui executar la cel·la per tal de comprovar-ho, simplement cal descomentar-la."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Perceptró Multicapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "\n",
    "from keras.optimizers import SGD, Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris, fetch_covtype, fetch_california_housing\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funció per construir el model\n",
    "def build_model(n_layers, n_units, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_units, activation='relu', input_shape=(X_train_normalized.shape[1],)))\n",
    "    for _ in range(n_layers - 1):\n",
    "        model.add(Dense(n_units, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "# Funció per a visualitzar les corbes d'aprenentatge\n",
    "def plot_curves(history):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mean_squared_error'], label='Train MSE')\n",
    "    plt.plot(history.history['val_mean_squared_error'], label='Validation MSE')\n",
    "    plt.title('MSE Curves')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Procés iteratiu\n",
    "n_iterations = 4\n",
    "n_units = 64\n",
    "batch_sizes = [64, 32, 16, 8]\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    print(f\"Iteració {i+1}\")\n",
    "    model = build_model(i + 1, n_units, learning_rate=0.001)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    history = model.fit(X_train_normalized, y_train, epochs=1000, batch_size=batch_sizes[i], validation_split=0.1, verbose=2, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evalua el model\n",
    "    loss, mse = model.evaluate(X_test_normalized, y_test, verbose=1)\n",
    "    y_pred = model.predict(X_test_normalized)\n",
    "    r2 = r2_score(y_test, y_pred.flatten())\n",
    "    print(f\"Test Loss: {loss}, Test MSE: {mse}, R2 Score: {r2}\")\n",
    "    \n",
    "    # Visualitza les corbes d'aprenentatge\n",
    "    plot_curves(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Defineix l'arquitectura del model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_normalized.shape[1],)), \n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')  # Capa de sortida per a regressió\n",
    "])\n",
    "\n",
    "# Defineix l'optimitzador i la taxa d'aprenentatge\n",
    "learning_rate = 0.001  # Taxa més petita per a regressió, per promoure una convergència més suau\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compila el model\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "# Entrena el model\n",
    "num_epochs = 1000\n",
    "batch_size = 32  # Batch size més petit per millorar la generalització\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)  # Early stopping per evitar overfitting\n",
    "history = model.fit(X_train_normalized, y_train, epochs=num_epochs, batch_size=batch_size, validation_split=0.1, verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "# Avalua el model\n",
    "loss, mse = model.evaluate(X_test_normalized, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test MSE:\", mse)\n",
    "\n",
    "# Fer prediccions amb les dades de prova\n",
    "y_pred = model.predict(X_test_normalized)\n",
    "\n",
    "# Calcular i imprimir el R2 score\n",
    "r2 = r2_score(y_test, y_pred.flatten())  # Assegurar-se que les dimensions coincideixen\n",
    "print(f\"R2 Score: {r2}\")\n",
    "\n",
    "# Funció per a visualitzar les corbes d'aprenentatge\n",
    "def plot_curves(history):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mean_squared_error'], label='Train MSE')\n",
    "    plt.plot(history.history['val_mean_squared_error'], label='Validation MSE')\n",
    "    plt.title('MSE Curves')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Visualitza les corbes d'aprenentatge\n",
    "plot_curves(history)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
